# Sketch Recogonition Project

## Overview

Transformer-based language models are known to be good at predicting missing data even with incomplete context. Some language models have been introduced in the sketch recognition field, but there is no direct study yet investigating whether the next-token-prediction ability of language models help them excel with incomplete sketches. This project aims to fill this gap and provide insight on how the attention mechanism of language models contribute to their performance in the sketch recogonition field.

## Tasks

- [ ] Baseline models: Search for and implement baseline sketch recognition models. They might be traditional feature-based models, or conventional CNN vision models.
- [ ] Target models: Search for and implement transformer-based sketch recognition models. For example, Sketch-bert.
- [ ] Dataset: Build dataset of incomplete sketches, likely from [Quick, Draw!](https://quickdraw.withgoogle.com/).
- [ ] Experiment: Evaluate and compare performances of baseline models and target models, on the built dataset.

## Deliverables

- [ ] Report
- [ ] Code
- [ ] Slides
- [ ] Video
